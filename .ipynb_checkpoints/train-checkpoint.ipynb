{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import inference\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 处理好之后的数据文件。\n",
    "INPUT_DATA = './dataset/orl_faces.npy'\n",
    "# 保存训练好的模型的路径。\n",
    "MODEL_SAVE_PATH = './model/'\n",
    "MODEL_NAME = 'face_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(images, labels, num_examples, batch_size, index_in_epoch):\n",
    "    x = []\n",
    "    y = []\n",
    "    next_index = index_in_epoch + batch_size\n",
    "    if next_index > num_examples:\n",
    "        next_index = next_index % num_examples\n",
    "        x1 = images[index_in_epoch:]\n",
    "        x2 = images[:next_index]\n",
    "        y1 = labels[index_in_epoch:]\n",
    "        y2 = labels[:next_index]\n",
    "        x_ = np.vstack((x1, x2))\n",
    "        y_ = np.vstack((y1, y2))\n",
    "    elif next_index < num_examples:\n",
    "        x.append(images[index_in_epoch:next_index])\n",
    "        y.append(labels[index_in_epoch:next_index])\n",
    "        x_ = np.asarray(x)\n",
    "        y_ = np.asarray(y)\n",
    "    else:\n",
    "        next_index = next_index % num_examples\n",
    "        x.append(images[index_in_epoch:])\n",
    "        y.append(labels[index_in_epoch:])\n",
    "        x_ = np.asarray(x)\n",
    "        y_ = np.asarray(y)\n",
    "    index_in_epoch = next_index\n",
    "    \n",
    "    return (x_, y_, index_in_epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "#     print(type(training_data[0]))\n",
    "    index_in_epoch = 0\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            inference.IMAGE_SIZE,\n",
    "            inference.IMAGE_SIZE,\n",
    "            inference.NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, inference.OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference.inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    print(\"11111111111\")\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        len(training_data[0]) / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    print('222222222222222')\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    print('3333333333333333')\n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "#         print('initlized')\n",
    "        print('444444444444444')\n",
    "    \n",
    "    \n",
    "#----------------------------------------    \n",
    "    \n",
    "#         for i in range(TRAINING_STEPS):\n",
    "#             print('5555555555-------',i)\n",
    "# #             xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "#             xs, ys, index_in_epoch= next_batch(training_data[0],training_data[1],len(training_data[0]), BATCH_SIZE, index_in_epoch)\n",
    "#             reshaped_xs = np.reshape(xs, (\n",
    "#                 BATCH_SIZE,\n",
    "#                 inference.IMAGE_SIZE,\n",
    "#                 inference.IMAGE_SIZE,\n",
    "#                 inference.NUM_CHANNELS))\n",
    "#             _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "#----------------------------------------------------------            \n",
    "        start = 0\n",
    "        end = BATCH_SIZE\n",
    "        n_training_example = len(training_data[0])\n",
    "        for i in range(TRAINING_STEPS):\n",
    "#             xs = training_data[0][start:end]\n",
    "#             ys = training_data[1][start:end]\n",
    "#             start = end\n",
    "#             if start == n_training_example:\n",
    "#                 start = 0\n",
    "#             end = start + BATCH_SIZE\n",
    "#             if end > n_training_example: \n",
    "#                 end = end - n_training_example\n",
    "#                 xs = np.vstack((xs, training_data[0][:end]))\n",
    "#                 ys = np.vstack((ys, training_data[1][:end]))\n",
    "            if start > end:\n",
    "                x1 = training_data[0][start:]\n",
    "                x2 = training_data[0][:end]\n",
    "                y1 = training_data[1][start:]\n",
    "                y2 = training_data[1][:end]\n",
    "                xs = np.vstack((x1, x2))\n",
    "                ys = np.vstack((y1, y2))\n",
    "            else:\n",
    "                xs = training_data[0][start:end]\n",
    "                ys = training_data[1][start:end]\n",
    "            start = end\n",
    "            end = end + BATCH_SIZE\n",
    "            if start == n_training_example:\n",
    "                start = 0\n",
    "            if end > n_training_example:\n",
    "                end = end - n_training_example\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                inference.IMAGE_SIZE,\n",
    "                inference.IMAGE_SIZE,\n",
    "                inference.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "#--------------------------------------------------\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step=global_step)\n",
    "#             print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 training examples has loaded.\n",
      "<class 'numpy.ndarray'>\n",
      "11111111111\n",
      "222222222222222\n",
      "3333333333333333\n",
      "444444444444444\n",
      "After 1 training step(s), loss on training batch is 31.0797.\n",
      "After 101 training step(s), loss on training batch is 12.9251.\n",
      "After 201 training step(s), loss on training batch is 11.1264.\n",
      "After 301 training step(s), loss on training batch is 10.1409.\n",
      "After 401 training step(s), loss on training batch is 10.0083.\n",
      "After 501 training step(s), loss on training batch is 9.96771.\n",
      "After 601 training step(s), loss on training batch is 9.98302.\n",
      "After 701 training step(s), loss on training batch is 9.95754.\n",
      "After 801 training step(s), loss on training batch is 9.94758.\n",
      "After 901 training step(s), loss on training batch is 9.94958.\n",
      "After 1001 training step(s), loss on training batch is 9.94249.\n",
      "After 1101 training step(s), loss on training batch is 9.94256.\n",
      "After 1201 training step(s), loss on training batch is 9.94178.\n",
      "After 1301 training step(s), loss on training batch is 9.94594.\n",
      "After 1401 training step(s), loss on training batch is 9.94115.\n",
      "After 1501 training step(s), loss on training batch is 9.93704.\n",
      "After 1601 training step(s), loss on training batch is 9.93889.\n",
      "After 1701 training step(s), loss on training batch is 9.93547.\n",
      "After 1801 training step(s), loss on training batch is 9.93713.\n",
      "After 1901 training step(s), loss on training batch is 9.93613.\n",
      "After 2001 training step(s), loss on training batch is 9.93824.\n",
      "After 2101 training step(s), loss on training batch is 9.93592.\n",
      "After 2201 training step(s), loss on training batch is 9.93301.\n",
      "After 2301 training step(s), loss on training batch is 9.93457.\n",
      "After 2401 training step(s), loss on training batch is 9.93223.\n",
      "After 2501 training step(s), loss on training batch is 9.93472.\n",
      "After 2601 training step(s), loss on training batch is 9.93398.\n",
      "After 2701 training step(s), loss on training batch is 9.93516.\n",
      "After 2801 training step(s), loss on training batch is 9.93337.\n",
      "After 2901 training step(s), loss on training batch is 9.93212.\n",
      "After 3001 training step(s), loss on training batch is 9.93212.\n",
      "After 3101 training step(s), loss on training batch is 9.93126.\n",
      "After 3201 training step(s), loss on training batch is 9.93217.\n",
      "After 3301 training step(s), loss on training batch is 9.93318.\n",
      "After 3401 training step(s), loss on training batch is 9.93222.\n",
      "After 3501 training step(s), loss on training batch is 9.93242.\n",
      "After 3601 training step(s), loss on training batch is 9.93135.\n",
      "After 3701 training step(s), loss on training batch is 9.93067.\n",
      "After 3801 training step(s), loss on training batch is 9.9306.\n",
      "After 3901 training step(s), loss on training batch is 9.93169.\n",
      "After 4001 training step(s), loss on training batch is 9.93323.\n",
      "After 4101 training step(s), loss on training batch is 9.93255.\n",
      "After 4201 training step(s), loss on training batch is 9.93108.\n",
      "After 4301 training step(s), loss on training batch is 9.93157.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ebb1f66457b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d training examples has loaded.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_training_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-71db4e3b51ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 inference.NUM_CHANNELS))\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreshaped_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;31m#--------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    photo_data = np.load(INPUT_DATA)\n",
    "    training_images = np.asarray(photo_data[0])\n",
    "    training_labels = np.asarray(photo_data[1])\n",
    "    n_training_example = len(training_images)\n",
    "    print(\"%d training examples has loaded.\" % (n_training_example))\n",
    "    print(type(training_images))\n",
    "    train([training_images, training_labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
